{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 234
    },
    "colab_type": "code",
    "id": "aNXH9i6zKIxD",
    "outputId": "8f9955b2-f996-4bc6-dcb7-833dfb13809f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras-vggface in c:\\users\\nico9\\anaconda3\\envs\\gputest\\lib\\site-packages (0.6)\n",
      "Requirement already satisfied: scipy>=0.14 in c:\\users\\nico9\\appdata\\roaming\\python\\python37\\site-packages (from keras-vggface) (1.4.1)\n",
      "Requirement already satisfied: h5py in c:\\users\\nico9\\appdata\\roaming\\python\\python37\\site-packages (from keras-vggface) (2.10.0)\n",
      "Requirement already satisfied: pillow in c:\\users\\nico9\\anaconda3\\envs\\gputest\\lib\\site-packages (from keras-vggface) (7.1.2)\n",
      "Requirement already satisfied: keras in c:\\users\\nico9\\anaconda3\\envs\\gputest\\lib\\site-packages (from keras-vggface) (2.3.1)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\nico9\\appdata\\roaming\\python\\python37\\site-packages (from keras-vggface) (1.14.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\nico9\\anaconda3\\envs\\gputest\\lib\\site-packages (from keras-vggface) (5.3.1)\n",
      "Requirement already satisfied: numpy>=1.9.1 in c:\\users\\nico9\\appdata\\roaming\\python\\python37\\site-packages (from keras-vggface) (1.18.4)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in c:\\users\\nico9\\appdata\\roaming\\python\\python37\\site-packages (from keras->keras-vggface) (1.1.2)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in c:\\users\\nico9\\anaconda3\\envs\\gputest\\lib\\site-packages (from keras->keras-vggface) (1.0.8)\n"
     ]
    }
   ],
   "source": [
    "!pip install keras-vggface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "uja7Om2eJ69K",
    "outputId": "a1329cda-4dea-4bea-bdc5-50763619ed01"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras_vggface.vggface import VGGFace\n",
    "from keras.models import Model\n",
    "from keras.layers import Flatten, Dense, Dropout, Conv2D, Input, MaxPooling2D, Activation\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import random\n",
    "from keras.optimizers import SGD\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras import backend as K\n",
    "from keras.constraints import Constraint\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "7AzMmAOUKMSZ",
    "outputId": "06655017-64f1-47d2-eef6-747386873422"
   },
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 224\n",
    "vggface = VGGFace(model='vgg16', input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "EZQJe5GQKW6V",
    "outputId": "c34b1657-1dfc-4e73-b381-f2ce8793c4cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.7 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv1_1 (Conv2D)             (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "conv1_2 (Conv2D)             (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "pool1 (MaxPooling2D)         (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2_1 (Conv2D)             (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "conv2_2 (Conv2D)             (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "pool2 (MaxPooling2D)         (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv3_1 (Conv2D)             (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "conv3_2 (Conv2D)             (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "conv3_3 (Conv2D)             (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "pool3 (MaxPooling2D)         (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv4_1 (Conv2D)             (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "conv4_2 (Conv2D)             (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "conv4_3 (Conv2D)             (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "pool4 (MaxPooling2D)         (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv5_1 (Conv2D)             (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "conv5_2 (Conv2D)             (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "conv5_3 (Conv2D)             (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "pool5 (MaxPooling2D)         (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "fc6 (Dense)                  (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "fc6/relu (Activation)        (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "Dropout (Dropout)            (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "fc7 (Dense)                  (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "fc7/relu (Activation)        (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 7)                 28679     \n",
      "=================================================================\n",
      "Total params: 134,289,223\n",
      "Trainable params: 119,574,535\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vggface.layers.pop()\n",
    "vggface.layers.pop()\n",
    "layers = [l for l in vggface.layers]\n",
    "for i in range(0, 18):\n",
    "     layers[i].trainable = False\n",
    "\n",
    "x = Dropout(0.7, name = 'Dropout')(layers[21].output)\n",
    "for i in range(22, len(layers)):\n",
    "    x = layers[i](x)\n",
    "\n",
    "x = Dense(7, activation = 'softmax')(x)    \n",
    "model = Model(inputs = layers[0].input, outputs = x)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e-ClF-69Kerv"
   },
   "outputs": [],
   "source": [
    "train_folder = \"C:/Users/nico9/Desktop/FaceDetection/Train\"\n",
    "val_folder = \"C:/Users/nico9/Desktop/FaceDetection/Val\"\n",
    "\n",
    "train_datagen = ImageDataGenerator(preprocessing_function=preprocess_input, horizontal_flip=True)\n",
    "val_datagen = ImageDataGenerator(preprocessing_function=preprocess_input, horizontal_flip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "0BVgxQtzK00R",
    "outputId": "6e5318d7-ae44-49bc-8ac6-81d23014427b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28709 images belonging to 7 classes.\n",
      "Found 3589 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "train_batchsize = 32\n",
    "val_batchsize = 8\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_folder,\n",
    "        target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "        batch_size=train_batchsize,\n",
    "        class_mode=\"categorical\"\n",
    ")\n",
    "\n",
    "val_generator = val_datagen.flow_from_directory(\n",
    "        val_folder,\n",
    "        target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "        batch_size=val_batchsize,\n",
    "        class_mode=\"categorical\",\n",
    "        shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tTZj0BueK3Qu"
   },
   "outputs": [],
   "source": [
    "reduce_lr = ReduceLROnPlateau(monitor = 'val_loss', factor = 0.1 , patience = 10)\n",
    "\n",
    "callbacks_list = [reduce_lr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3ng28XliK5ZY"
   },
   "outputs": [],
   "source": [
    "num_epochs = 200\n",
    "learning_rate = 1e-4\n",
    "sgd = SGD(lr=learning_rate, momentum = 0.9)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=sgd, metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 470
    },
    "colab_type": "code",
    "id": "8ubCRRmLK7Xd",
    "outputId": "6f102321-801c-401a-abd4-1f5a9391a2a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "898/898 [==============================] - 261s 291ms/step - loss: 1.8089 - acc: 0.4421 - val_loss: 1.7915 - val_acc: 0.5300\n",
      "Epoch 2/200\n",
      "898/898 [==============================] - 256s 285ms/step - loss: 1.2680 - acc: 0.5426 - val_loss: 1.3672 - val_acc: 0.5620\n",
      "Epoch 3/200\n",
      "898/898 [==============================] - 256s 285ms/step - loss: 1.1419 - acc: 0.5837 - val_loss: 1.4073 - val_acc: 0.5865\n",
      "Epoch 4/200\n",
      "898/898 [==============================] - 256s 285ms/step - loss: 1.0380 - acc: 0.6182 - val_loss: 1.4830 - val_acc: 0.6002\n",
      "Epoch 5/200\n",
      "898/898 [==============================] - 256s 285ms/step - loss: 0.9708 - acc: 0.6462 - val_loss: 1.3208 - val_acc: 0.6066\n",
      "Epoch 6/200\n",
      "898/898 [==============================] - 255s 285ms/step - loss: 0.9058 - acc: 0.6691 - val_loss: 1.4162 - val_acc: 0.6194\n",
      "Epoch 7/200\n",
      "898/898 [==============================] - 255s 285ms/step - loss: 0.8498 - acc: 0.6874 - val_loss: 1.5796 - val_acc: 0.6247\n",
      "Epoch 8/200\n",
      "898/898 [==============================] - 256s 285ms/step - loss: 0.8125 - acc: 0.7018 - val_loss: 1.4552 - val_acc: 0.6252\n",
      "Epoch 9/200\n",
      "898/898 [==============================] - 256s 285ms/step - loss: 0.7630 - acc: 0.7224 - val_loss: 1.2937 - val_acc: 0.6319\n",
      "Epoch 10/200\n",
      "898/898 [==============================] - 256s 285ms/step - loss: 0.7255 - acc: 0.7367 - val_loss: 1.4637 - val_acc: 0.6406\n",
      "Epoch 11/200\n",
      "898/898 [==============================] - 256s 285ms/step - loss: 0.6776 - acc: 0.7540 - val_loss: 1.1185 - val_acc: 0.6431\n",
      "Epoch 12/200\n",
      "898/898 [==============================] - 256s 285ms/step - loss: 0.6495 - acc: 0.7634 - val_loss: 1.4612 - val_acc: 0.6411\n",
      "Epoch 13/200\n",
      "898/898 [==============================] - 256s 285ms/step - loss: 0.6138 - acc: 0.7786 - val_loss: 1.3305 - val_acc: 0.6447\n",
      "Epoch 14/200\n",
      "898/898 [==============================] - 255s 284ms/step - loss: 0.5831 - acc: 0.7891 - val_loss: 1.2691 - val_acc: 0.6528\n",
      "Epoch 15/200\n",
      "898/898 [==============================] - 256s 285ms/step - loss: 0.5565 - acc: 0.7992 - val_loss: 1.5375 - val_acc: 0.6553\n",
      "Epoch 16/200\n",
      "898/898 [==============================] - 256s 285ms/step - loss: 0.5250 - acc: 0.8144 - val_loss: 1.5503 - val_acc: 0.6481\n",
      "Epoch 17/200\n",
      "898/898 [==============================] - 255s 284ms/step - loss: 0.4966 - acc: 0.8234 - val_loss: 1.4022 - val_acc: 0.6598\n",
      "Epoch 18/200\n",
      "898/898 [==============================] - 255s 284ms/step - loss: 0.4764 - acc: 0.8283 - val_loss: 1.4331 - val_acc: 0.6578\n",
      "Epoch 19/200\n",
      "898/898 [==============================] - 255s 284ms/step - loss: 0.4509 - acc: 0.8370 - val_loss: 1.7924 - val_acc: 0.6634\n",
      "Epoch 20/200\n",
      "898/898 [==============================] - 255s 284ms/step - loss: 0.4259 - acc: 0.8480 - val_loss: 1.8419 - val_acc: 0.6670\n",
      "Epoch 21/200\n",
      "898/898 [==============================] - 255s 284ms/step - loss: 0.4079 - acc: 0.8565 - val_loss: 1.4378 - val_acc: 0.6606\n",
      "Epoch 22/200\n",
      "898/898 [==============================] - 255s 284ms/step - loss: 0.3694 - acc: 0.8708 - val_loss: 1.8210 - val_acc: 0.6665\n",
      "Epoch 23/200\n",
      "898/898 [==============================] - 255s 284ms/step - loss: 0.3574 - acc: 0.8754 - val_loss: 1.5981 - val_acc: 0.6698\n",
      "Epoch 24/200\n",
      "898/898 [==============================] - 255s 284ms/step - loss: 0.3526 - acc: 0.8777 - val_loss: 1.9182 - val_acc: 0.6695\n",
      "Epoch 25/200\n",
      "898/898 [==============================] - 255s 285ms/step - loss: 0.3536 - acc: 0.8791 - val_loss: 1.9197 - val_acc: 0.6704\n",
      "Epoch 26/200\n",
      "898/898 [==============================] - 255s 284ms/step - loss: 0.3450 - acc: 0.8827 - val_loss: 1.8849 - val_acc: 0.6695\n",
      "Epoch 27/200\n",
      "898/898 [==============================] - 255s 284ms/step - loss: 0.3349 - acc: 0.8857 - val_loss: 1.6061 - val_acc: 0.6651\n",
      "Epoch 28/200\n",
      "898/898 [==============================] - 255s 284ms/step - loss: 0.3375 - acc: 0.8830 - val_loss: 1.9185 - val_acc: 0.6743\n",
      "Epoch 29/200\n",
      "898/898 [==============================] - 256s 285ms/step - loss: 0.3331 - acc: 0.8872 - val_loss: 1.5606 - val_acc: 0.6668\n",
      "Epoch 30/200\n",
      "898/898 [==============================] - 255s 285ms/step - loss: 0.3343 - acc: 0.8852 - val_loss: 1.8766 - val_acc: 0.6701\n",
      "Epoch 31/200\n",
      "898/898 [==============================] - 256s 285ms/step - loss: 0.3245 - acc: 0.8896 - val_loss: 1.8876 - val_acc: 0.6687\n",
      "Epoch 32/200\n",
      "898/898 [==============================] - 255s 284ms/step - loss: 0.3272 - acc: 0.8883 - val_loss: 1.6260 - val_acc: 0.6643\n",
      "Epoch 33/200\n",
      "898/898 [==============================] - 255s 284ms/step - loss: 0.3298 - acc: 0.8856 - val_loss: 1.8730 - val_acc: 0.6659\n",
      "Epoch 34/200\n",
      "898/898 [==============================] - 256s 285ms/step - loss: 0.3210 - acc: 0.8908 - val_loss: 1.5849 - val_acc: 0.6679\n",
      "Epoch 35/200\n",
      "898/898 [==============================] - 256s 285ms/step - loss: 0.3170 - acc: 0.8956 - val_loss: 1.9080 - val_acc: 0.6684\n",
      "Epoch 36/200\n",
      "898/898 [==============================] - 255s 284ms/step - loss: 0.3254 - acc: 0.8887 - val_loss: 1.5856 - val_acc: 0.6668\n",
      "Epoch 37/200\n",
      "898/898 [==============================] - 256s 285ms/step - loss: 0.3215 - acc: 0.8904 - val_loss: 1.8746 - val_acc: 0.6682\n",
      "Epoch 38/200\n",
      "898/898 [==============================] - 255s 284ms/step - loss: 0.3217 - acc: 0.8904 - val_loss: 1.8764 - val_acc: 0.6612\n",
      "Epoch 39/200\n",
      "898/898 [==============================] - 256s 285ms/step - loss: 0.3174 - acc: 0.8926 - val_loss: 1.5868 - val_acc: 0.6723\n",
      "Epoch 40/200\n",
      "898/898 [==============================] - 256s 285ms/step - loss: 0.3180 - acc: 0.8913 - val_loss: 1.8785 - val_acc: 0.6721\n",
      "Epoch 41/200\n",
      "898/898 [==============================] - 255s 285ms/step - loss: 0.3184 - acc: 0.8901 - val_loss: 1.9074 - val_acc: 0.6645\n",
      "Epoch 42/200\n",
      "898/898 [==============================] - 255s 284ms/step - loss: 0.3235 - acc: 0.8874 - val_loss: 1.8761 - val_acc: 0.6687\n",
      "Epoch 43/200\n",
      "898/898 [==============================] - 256s 285ms/step - loss: 0.3197 - acc: 0.8931 - val_loss: 1.6205 - val_acc: 0.6687\n",
      "Epoch 44/200\n",
      "898/898 [==============================] - 256s 285ms/step - loss: 0.3229 - acc: 0.8911 - val_loss: 1.5890 - val_acc: 0.6698\n",
      "Epoch 45/200\n",
      "898/898 [==============================] - 256s 285ms/step - loss: 0.3141 - acc: 0.8936 - val_loss: 1.8799 - val_acc: 0.6723\n",
      "Epoch 46/200\n",
      "898/898 [==============================] - 256s 285ms/step - loss: 0.3184 - acc: 0.8925 - val_loss: 1.8779 - val_acc: 0.6740\n",
      "Epoch 47/200\n",
      "898/898 [==============================] - 256s 285ms/step - loss: 0.3211 - acc: 0.8899 - val_loss: 1.9063 - val_acc: 0.6648\n",
      "Epoch 48/200\n",
      "898/898 [==============================] - 255s 285ms/step - loss: 0.3196 - acc: 0.8898 - val_loss: 1.6211 - val_acc: 0.6698\n",
      "Epoch 49/200\n",
      "898/898 [==============================] - 256s 285ms/step - loss: 0.3190 - acc: 0.8913 - val_loss: 1.5863 - val_acc: 0.6679\n",
      "Epoch 50/200\n",
      "898/898 [==============================] - 255s 284ms/step - loss: 0.3196 - acc: 0.8923 - val_loss: 1.9108 - val_acc: 0.6662\n",
      "Epoch 51/200\n",
      "898/898 [==============================] - 255s 284ms/step - loss: 0.3249 - acc: 0.8884 - val_loss: 1.6237 - val_acc: 0.6690\n",
      "Epoch 52/200\n",
      "898/898 [==============================] - 256s 285ms/step - loss: 0.3165 - acc: 0.8941 - val_loss: 1.9095 - val_acc: 0.6651\n",
      "Epoch 53/200\n",
      "898/898 [==============================] - 256s 285ms/step - loss: 0.3193 - acc: 0.8910 - val_loss: 1.9123 - val_acc: 0.6682\n",
      "Epoch 54/200\n",
      "898/898 [==============================] - 255s 284ms/step - loss: 0.3234 - acc: 0.8892 - val_loss: 1.9075 - val_acc: 0.6693\n",
      "Epoch 55/200\n",
      "898/898 [==============================] - 255s 284ms/step - loss: 0.3152 - acc: 0.8940 - val_loss: 1.9075 - val_acc: 0.6659\n",
      "Epoch 56/200\n",
      "898/898 [==============================] - 255s 284ms/step - loss: 0.3220 - acc: 0.8897 - val_loss: 1.9129 - val_acc: 0.6654\n",
      "Epoch 57/200\n",
      "898/898 [==============================] - 255s 284ms/step - loss: 0.3201 - acc: 0.8890 - val_loss: 1.8810 - val_acc: 0.6662\n",
      "Epoch 58/200\n",
      "898/898 [==============================] - 256s 285ms/step - loss: 0.3128 - acc: 0.8951 - val_loss: 1.6232 - val_acc: 0.6643\n",
      "Epoch 59/200\n",
      "898/898 [==============================] - 255s 284ms/step - loss: 0.3225 - acc: 0.8901 - val_loss: 1.5884 - val_acc: 0.6698\n",
      "Epoch 60/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "898/898 [==============================] - 256s 285ms/step - loss: 0.3196 - acc: 0.8921 - val_loss: 1.8776 - val_acc: 0.6698\n",
      "Epoch 61/200\n",
      "898/898 [==============================] - 256s 285ms/step - loss: 0.3216 - acc: 0.8904 - val_loss: 1.5878 - val_acc: 0.6659\n",
      "Epoch 62/200\n",
      "898/898 [==============================] - 256s 285ms/step - loss: 0.3179 - acc: 0.8922 - val_loss: 1.5892 - val_acc: 0.6651\n",
      "Epoch 63/200\n",
      "898/898 [==============================] - 256s 285ms/step - loss: 0.3168 - acc: 0.8924 - val_loss: 1.9074 - val_acc: 0.6721\n",
      "Epoch 64/200\n",
      "898/898 [==============================] - 255s 284ms/step - loss: 0.3201 - acc: 0.8922 - val_loss: 1.9068 - val_acc: 0.6726\n",
      "Epoch 65/200\n",
      "898/898 [==============================] - 255s 285ms/step - loss: 0.3225 - acc: 0.8910 - val_loss: 1.8775 - val_acc: 0.6668\n",
      "Epoch 66/200\n",
      "898/898 [==============================] - 256s 285ms/step - loss: 0.3214 - acc: 0.8874 - val_loss: 1.5878 - val_acc: 0.6687\n",
      "Epoch 67/200\n",
      "898/898 [==============================] - 256s 285ms/step - loss: 0.3193 - acc: 0.8907 - val_loss: 1.9122 - val_acc: 0.6715\n",
      "Epoch 68/200\n",
      "898/898 [==============================] - 256s 285ms/step - loss: 0.3181 - acc: 0.8924 - val_loss: 1.9102 - val_acc: 0.6654\n",
      "Epoch 69/200\n",
      "898/898 [==============================] - 256s 285ms/step - loss: 0.3175 - acc: 0.8926 - val_loss: 1.9094 - val_acc: 0.6665\n",
      "Epoch 70/200\n",
      "898/898 [==============================] - 255s 284ms/step - loss: 0.3192 - acc: 0.8907 - val_loss: 1.8749 - val_acc: 0.6723\n",
      "Epoch 71/200\n",
      "898/898 [==============================] - 255s 284ms/step - loss: 0.3191 - acc: 0.8913 - val_loss: 1.6237 - val_acc: 0.6721\n",
      "Epoch 72/200\n",
      "898/898 [==============================] - 255s 284ms/step - loss: 0.3224 - acc: 0.8897 - val_loss: 1.5918 - val_acc: 0.6648\n",
      "Epoch 73/200\n",
      "898/898 [==============================] - 255s 284ms/step - loss: 0.3190 - acc: 0.8895 - val_loss: 1.5918 - val_acc: 0.6701\n",
      "Epoch 74/200\n",
      "898/898 [==============================] - 255s 284ms/step - loss: 0.3211 - acc: 0.8906 - val_loss: 1.5912 - val_acc: 0.6637\n",
      "Epoch 75/200\n",
      "898/898 [==============================] - 255s 284ms/step - loss: 0.3234 - acc: 0.8885 - val_loss: 1.8803 - val_acc: 0.6643\n",
      "Epoch 76/200\n",
      "898/898 [==============================] - 255s 284ms/step - loss: 0.3194 - acc: 0.8900 - val_loss: 1.5898 - val_acc: 0.6707\n",
      "Epoch 77/200\n",
      "898/898 [==============================] - 255s 284ms/step - loss: 0.3192 - acc: 0.8905 - val_loss: 1.6183 - val_acc: 0.6626\n",
      "Epoch 78/200\n",
      "898/898 [==============================] - 256s 285ms/step - loss: 0.3200 - acc: 0.8911 - val_loss: 1.5864 - val_acc: 0.6687\n",
      "Epoch 79/200\n",
      "898/898 [==============================] - 256s 285ms/step - loss: 0.3255 - acc: 0.8889 - val_loss: 1.9128 - val_acc: 0.6659\n",
      "Epoch 80/200\n",
      "898/898 [==============================] - 256s 285ms/step - loss: 0.3183 - acc: 0.8919 - val_loss: 1.5912 - val_acc: 0.6631\n",
      "Epoch 81/200\n",
      "898/898 [==============================] - 256s 285ms/step - loss: 0.3247 - acc: 0.8890 - val_loss: 1.5918 - val_acc: 0.6707\n",
      "Epoch 82/200\n",
      "898/898 [==============================] - 256s 285ms/step - loss: 0.3222 - acc: 0.8890 - val_loss: 1.6231 - val_acc: 0.6679\n",
      "Epoch 83/200\n",
      "898/898 [==============================] - 256s 285ms/step - loss: 0.3213 - acc: 0.8908 - val_loss: 1.9089 - val_acc: 0.6709\n",
      "Epoch 84/200\n",
      "898/898 [==============================] - 256s 285ms/step - loss: 0.3207 - acc: 0.8898 - val_loss: 1.5898 - val_acc: 0.6729\n",
      "Epoch 85/200\n",
      "898/898 [==============================] - 256s 285ms/step - loss: 0.3161 - acc: 0.8924 - val_loss: 1.6177 - val_acc: 0.6668\n",
      "Epoch 86/200\n",
      "898/898 [==============================] - 255s 284ms/step - loss: 0.3206 - acc: 0.8907 - val_loss: 1.6217 - val_acc: 0.6654\n",
      "Epoch 87/200\n",
      "898/898 [==============================] - 256s 285ms/step - loss: 0.3174 - acc: 0.8925 - val_loss: 1.6197 - val_acc: 0.6648\n",
      "Epoch 88/200\n",
      "898/898 [==============================] - 255s 284ms/step - loss: 0.3204 - acc: 0.8911 - val_loss: 1.9102 - val_acc: 0.6665\n",
      "Epoch 89/200\n",
      "898/898 [==============================] - 256s 285ms/step - loss: 0.3188 - acc: 0.8921 - val_loss: 1.6183 - val_acc: 0.6673\n",
      "Epoch 90/200\n",
      "898/898 [==============================] - 255s 284ms/step - loss: 0.3174 - acc: 0.8928 - val_loss: 1.8769 - val_acc: 0.6668\n",
      "Epoch 91/200\n",
      "898/898 [==============================] - 255s 284ms/step - loss: 0.3177 - acc: 0.8921 - val_loss: 1.6203 - val_acc: 0.6690\n",
      "Epoch 92/200\n",
      "898/898 [==============================] - 255s 284ms/step - loss: 0.3204 - acc: 0.8887 - val_loss: 1.5858 - val_acc: 0.6695\n",
      "Epoch 93/200\n",
      "898/898 [==============================] - 256s 285ms/step - loss: 0.3242 - acc: 0.8898 - val_loss: 1.6211 - val_acc: 0.6676\n",
      "Epoch 94/200\n",
      "898/898 [==============================] - 255s 284ms/step - loss: 0.3174 - acc: 0.8937 - val_loss: 1.9128 - val_acc: 0.6693\n",
      "Epoch 95/200\n",
      "898/898 [==============================] - 255s 284ms/step - loss: 0.3211 - acc: 0.8902 - val_loss: 1.6203 - val_acc: 0.6684\n",
      "Epoch 96/200\n",
      "898/898 [==============================] - 255s 284ms/step - loss: 0.3203 - acc: 0.8909 - val_loss: 1.9128 - val_acc: 0.6665\n",
      "Epoch 97/200\n",
      "898/898 [==============================] - 255s 284ms/step - loss: 0.3189 - acc: 0.8911 - val_loss: 1.5858 - val_acc: 0.6662\n",
      "Epoch 98/200\n",
      "898/898 [==============================] - 256s 285ms/step - loss: 0.3223 - acc: 0.8906 - val_loss: 1.6183 - val_acc: 0.6684\n",
      "Epoch 99/200\n",
      "898/898 [==============================] - 255s 284ms/step - loss: 0.3198 - acc: 0.8897 - val_loss: 1.6231 - val_acc: 0.6668\n",
      "Epoch 100/200\n",
      "898/898 [==============================] - 255s 285ms/step - loss: 0.3177 - acc: 0.8924 - val_loss: 1.8755 - val_acc: 0.6695\n",
      "Epoch 101/200\n",
      "898/898 [==============================] - 255s 284ms/step - loss: 0.3190 - acc: 0.8940 - val_loss: 1.5912 - val_acc: 0.6698\n",
      "Epoch 102/200\n",
      "898/898 [==============================] - 255s 284ms/step - loss: 0.3186 - acc: 0.8925 - val_loss: 1.6197 - val_acc: 0.6707\n",
      "Epoch 103/200\n",
      "898/898 [==============================] - 255s 284ms/step - loss: 0.3170 - acc: 0.8922 - val_loss: 1.6237 - val_acc: 0.6668\n",
      "Epoch 104/200\n",
      "898/898 [==============================] - 255s 284ms/step - loss: 0.3189 - acc: 0.8898 - val_loss: 1.6237 - val_acc: 0.6687\n",
      "Epoch 105/200\n",
      "898/898 [==============================] - 255s 284ms/step - loss: 0.3175 - acc: 0.8924 - val_loss: 1.6217 - val_acc: 0.6654\n",
      "Epoch 106/200\n",
      "898/898 [==============================] - 255s 284ms/step - loss: 0.3171 - acc: 0.8907 - val_loss: 1.5878 - val_acc: 0.6668\n",
      "Epoch 107/200\n",
      "898/898 [==============================] - 255s 284ms/step - loss: 0.3219 - acc: 0.8909 - val_loss: 1.5884 - val_acc: 0.6718\n",
      "Epoch 108/200\n",
      "898/898 [==============================] - 255s 284ms/step - loss: 0.3202 - acc: 0.8911 - val_loss: 1.5864 - val_acc: 0.6687\n",
      "Epoch 109/200\n",
      "898/898 [==============================] - 255s 284ms/step - loss: 0.3222 - acc: 0.8894 - val_loss: 1.5858 - val_acc: 0.6679\n",
      "Epoch 110/200\n",
      "898/898 [==============================] - 255s 284ms/step - loss: 0.3117 - acc: 0.8956 - val_loss: 1.9108 - val_acc: 0.6701\n",
      "Epoch 111/200\n",
      "898/898 [==============================] - 255s 284ms/step - loss: 0.3203 - acc: 0.8933 - val_loss: 1.5858 - val_acc: 0.6665\n",
      "Epoch 112/200\n",
      "898/898 [==============================] - 255s 284ms/step - loss: 0.3229 - acc: 0.8896 - val_loss: 1.8783 - val_acc: 0.6698\n",
      "Epoch 113/200\n",
      "898/898 [==============================] - 255s 284ms/step - loss: 0.3178 - acc: 0.8906 - val_loss: 1.8775 - val_acc: 0.6687\n",
      "Epoch 114/200\n",
      "898/898 [==============================] - 255s 284ms/step - loss: 0.3200 - acc: 0.8920 - val_loss: 1.8809 - val_acc: 0.6682\n",
      "Epoch 115/200\n",
      "898/898 [==============================] - 255s 284ms/step - loss: 0.3185 - acc: 0.8901 - val_loss: 1.8783 - val_acc: 0.6721\n",
      "Epoch 116/200\n",
      "898/898 [==============================] - 255s 284ms/step - loss: 0.3163 - acc: 0.8932 - val_loss: 1.5892 - val_acc: 0.6668\n",
      "Epoch 117/200\n",
      "898/898 [==============================] - 255s 284ms/step - loss: 0.3144 - acc: 0.8948 - val_loss: 1.9122 - val_acc: 0.6668\n",
      "Epoch 118/200\n",
      "898/898 [==============================] - 255s 284ms/step - loss: 0.3229 - acc: 0.8905 - val_loss: 1.6231 - val_acc: 0.6715\n",
      "Epoch 119/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "898/898 [==============================] - 255s 284ms/step - loss: 0.3216 - acc: 0.8920 - val_loss: 1.5918 - val_acc: 0.6707\n",
      "Epoch 120/200\n",
      "898/898 [==============================] - 255s 284ms/step - loss: 0.3166 - acc: 0.8931 - val_loss: 1.8755 - val_acc: 0.6712\n",
      "Epoch 121/200\n",
      "898/898 [==============================] - 255s 284ms/step - loss: 0.3225 - acc: 0.8889 - val_loss: 1.8755 - val_acc: 0.6707\n",
      "Epoch 122/200\n",
      "898/898 [==============================] - 255s 284ms/step - loss: 0.3197 - acc: 0.8918 - val_loss: 1.6197 - val_acc: 0.6665\n",
      "Epoch 123/200\n",
      "898/898 [==============================] - 255s 284ms/step - loss: 0.3198 - acc: 0.8890 - val_loss: 1.8749 - val_acc: 0.6679\n",
      "Epoch 124/200\n",
      "898/898 [==============================] - 255s 284ms/step - loss: 0.3179 - acc: 0.8920 - val_loss: 1.8749 - val_acc: 0.6662\n",
      "Epoch 125/200\n",
      "898/898 [==============================] - 255s 284ms/step - loss: 0.3184 - acc: 0.8915 - val_loss: 1.6177 - val_acc: 0.6704\n",
      "Epoch 126/200\n",
      "898/898 [==============================] - 255s 284ms/step - loss: 0.3213 - acc: 0.8894 - val_loss: 1.5898 - val_acc: 0.6715\n",
      "Epoch 127/200\n",
      "898/898 [==============================] - 255s 284ms/step - loss: 0.3192 - acc: 0.8901 - val_loss: 1.9122 - val_acc: 0.6704\n",
      "Epoch 128/200\n",
      "898/898 [==============================] - 256s 285ms/step - loss: 0.3216 - acc: 0.8913 - val_loss: 1.9122 - val_acc: 0.6684\n",
      "Epoch 129/200\n",
      "898/898 [==============================] - 255s 284ms/step - loss: 0.3176 - acc: 0.8906 - val_loss: 1.8755 - val_acc: 0.6684\n",
      "Epoch 130/200\n",
      "898/898 [==============================] - 255s 284ms/step - loss: 0.3218 - acc: 0.8896 - val_loss: 1.5858 - val_acc: 0.6712\n",
      "Epoch 131/200\n",
      "898/898 [==============================] - 255s 284ms/step - loss: 0.3198 - acc: 0.8914 - val_loss: 1.5864 - val_acc: 0.6673\n",
      "Epoch 132/200\n",
      "898/898 [==============================] - 255s 284ms/step - loss: 0.3233 - acc: 0.8903 - val_loss: 1.5864 - val_acc: 0.6668\n",
      "Epoch 133/200\n",
      "898/898 [==============================] - 256s 285ms/step - loss: 0.3157 - acc: 0.8928 - val_loss: 1.8803 - val_acc: 0.6676\n",
      "Epoch 134/200\n",
      "898/898 [==============================] - 255s 284ms/step - loss: 0.3196 - acc: 0.8934 - val_loss: 1.5892 - val_acc: 0.6665\n",
      "Epoch 135/200\n",
      "898/898 [==============================] - 255s 285ms/step - loss: 0.3133 - acc: 0.8946 - val_loss: 1.8749 - val_acc: 0.6629\n",
      "Epoch 136/200\n",
      "898/898 [==============================] - 255s 284ms/step - loss: 0.3208 - acc: 0.8906 - val_loss: 1.6237 - val_acc: 0.6654\n",
      "Epoch 137/200\n",
      "898/898 [==============================] - 255s 284ms/step - loss: 0.3169 - acc: 0.8912 - val_loss: 1.8803 - val_acc: 0.6673\n",
      "Epoch 138/200\n",
      "898/898 [==============================] - 255s 284ms/step - loss: 0.3183 - acc: 0.8925 - val_loss: 1.6231 - val_acc: 0.6656\n",
      "Epoch 139/200\n",
      "898/898 [==============================] - 255s 284ms/step - loss: 0.3178 - acc: 0.8914 - val_loss: 1.9074 - val_acc: 0.6682\n",
      "Epoch 140/200\n",
      "898/898 [==============================] - 255s 284ms/step - loss: 0.3221 - acc: 0.8901 - val_loss: 1.6217 - val_acc: 0.6679\n",
      "Epoch 141/200\n",
      "898/898 [==============================] - 255s 284ms/step - loss: 0.3191 - acc: 0.8903 - val_loss: 1.9074 - val_acc: 0.6698\n",
      "Epoch 142/200\n",
      "898/898 [==============================] - 255s 284ms/step - loss: 0.3160 - acc: 0.8946 - val_loss: 1.8809 - val_acc: 0.6656\n",
      "Epoch 143/200\n",
      "898/898 [==============================] - 255s 284ms/step - loss: 0.3253 - acc: 0.8907 - val_loss: 1.5892 - val_acc: 0.6684\n",
      "Epoch 144/200\n",
      "898/898 [==============================] - 255s 284ms/step - loss: 0.3167 - acc: 0.8902 - val_loss: 1.9108 - val_acc: 0.6704\n",
      "Epoch 145/200\n",
      "898/898 [==============================] - 255s 284ms/step - loss: 0.3216 - acc: 0.8921 - val_loss: 1.8809 - val_acc: 0.6673\n",
      "Epoch 146/200\n",
      "898/898 [==============================] - 255s 284ms/step - loss: 0.3185 - acc: 0.8895 - val_loss: 1.8769 - val_acc: 0.6662\n",
      "Epoch 147/200\n",
      "898/898 [==============================] - 255s 284ms/step - loss: 0.3168 - acc: 0.8920 - val_loss: 1.8789 - val_acc: 0.6662\n",
      "Epoch 148/200\n",
      "898/898 [==============================] - 255s 284ms/step - loss: 0.3185 - acc: 0.8942 - val_loss: 1.8769 - val_acc: 0.6637\n",
      "Epoch 149/200\n",
      "898/898 [==============================] - 255s 284ms/step - loss: 0.3213 - acc: 0.8911 - val_loss: 1.5884 - val_acc: 0.6648\n",
      "Epoch 150/200\n",
      "898/898 [==============================] - 255s 284ms/step - loss: 0.3206 - acc: 0.8909 - val_loss: 1.6211 - val_acc: 0.6704\n",
      "Epoch 151/200\n",
      "898/898 [==============================] - 255s 284ms/step - loss: 0.3252 - acc: 0.8887 - val_loss: 1.5898 - val_acc: 0.6690\n",
      "Epoch 152/200\n",
      "898/898 [==============================] - 255s 284ms/step - loss: 0.3192 - acc: 0.8929 - val_loss: 1.9128 - val_acc: 0.6737\n",
      "Epoch 153/200\n",
      "898/898 [==============================] - 255s 284ms/step - loss: 0.3201 - acc: 0.8928 - val_loss: 1.5892 - val_acc: 0.6668\n",
      "Epoch 154/200\n",
      "898/898 [==============================] - 255s 284ms/step - loss: 0.3234 - acc: 0.8879 - val_loss: 1.9128 - val_acc: 0.6687\n",
      "Epoch 155/200\n",
      "898/898 [==============================] - 255s 284ms/step - loss: 0.3195 - acc: 0.8914 - val_loss: 1.8809 - val_acc: 0.6651\n",
      "Epoch 156/200\n",
      "898/898 [==============================] - 255s 284ms/step - loss: 0.3251 - acc: 0.8876 - val_loss: 1.5898 - val_acc: 0.6704\n",
      "Epoch 157/200\n",
      "898/898 [==============================] - 255s 284ms/step - loss: 0.3172 - acc: 0.8928 - val_loss: 1.5884 - val_acc: 0.6682\n",
      "Epoch 158/200\n",
      "898/898 [==============================] - 255s 284ms/step - loss: 0.3229 - acc: 0.8891 - val_loss: 1.8749 - val_acc: 0.6634\n",
      "Epoch 159/200\n",
      "898/898 [==============================] - 255s 284ms/step - loss: 0.3171 - acc: 0.8929 - val_loss: 1.6211 - val_acc: 0.6665\n",
      "Epoch 160/200\n",
      "898/898 [==============================] - 255s 284ms/step - loss: 0.3151 - acc: 0.8950 - val_loss: 1.6231 - val_acc: 0.6693\n",
      "Epoch 161/200\n",
      "898/898 [==============================] - 255s 284ms/step - loss: 0.3244 - acc: 0.8889 - val_loss: 1.9108 - val_acc: 0.6687\n",
      "Epoch 162/200\n",
      "898/898 [==============================] - 255s 284ms/step - loss: 0.3179 - acc: 0.8924 - val_loss: 1.5864 - val_acc: 0.6673\n",
      "Epoch 163/200\n",
      "898/898 [==============================] - 255s 284ms/step - loss: 0.3199 - acc: 0.8912 - val_loss: 1.5898 - val_acc: 0.6743\n",
      "Epoch 164/200\n",
      "898/898 [==============================] - 255s 284ms/step - loss: 0.3182 - acc: 0.8914 - val_loss: 1.6177 - val_acc: 0.6704\n",
      "Epoch 165/200\n",
      "898/898 [==============================] - 255s 284ms/step - loss: 0.3191 - acc: 0.8907 - val_loss: 1.5912 - val_acc: 0.6679\n",
      "Epoch 166/200\n",
      "898/898 [==============================] - 255s 284ms/step - loss: 0.3201 - acc: 0.8916 - val_loss: 1.8769 - val_acc: 0.6682\n",
      "Epoch 167/200\n",
      "898/898 [==============================] - 255s 284ms/step - loss: 0.3171 - acc: 0.8938 - val_loss: 1.9108 - val_acc: 0.6668\n",
      "Epoch 168/200\n",
      "898/898 [==============================] - 255s 284ms/step - loss: 0.3135 - acc: 0.8939 - val_loss: 1.8809 - val_acc: 0.6682\n",
      "Epoch 169/200\n",
      "898/898 [==============================] - 255s 284ms/step - loss: 0.3165 - acc: 0.8931 - val_loss: 1.6203 - val_acc: 0.6651\n",
      "Epoch 170/200\n",
      "898/898 [==============================] - 255s 284ms/step - loss: 0.3186 - acc: 0.8930 - val_loss: 1.9122 - val_acc: 0.6662\n",
      "Epoch 171/200\n",
      "898/898 [==============================] - 255s 284ms/step - loss: 0.3272 - acc: 0.8880 - val_loss: 1.5898 - val_acc: 0.6665\n",
      "Epoch 172/200\n",
      "898/898 [==============================] - 255s 284ms/step - loss: 0.3150 - acc: 0.8940 - val_loss: 1.9068 - val_acc: 0.6665\n",
      "Epoch 173/200\n",
      "898/898 [==============================] - 255s 284ms/step - loss: 0.3181 - acc: 0.8913 - val_loss: 1.8789 - val_acc: 0.6698\n",
      "Epoch 174/200\n",
      "898/898 [==============================] - 255s 284ms/step - loss: 0.3203 - acc: 0.8923 - val_loss: 1.5912 - val_acc: 0.6645\n",
      "Epoch 175/200\n",
      "898/898 [==============================] - 255s 284ms/step - loss: 0.3206 - acc: 0.8889 - val_loss: 1.6237 - val_acc: 0.6673\n",
      "Epoch 176/200\n",
      "898/898 [==============================] - 255s 284ms/step - loss: 0.3181 - acc: 0.8912 - val_loss: 1.5884 - val_acc: 0.6682\n",
      "Epoch 177/200\n",
      "898/898 [==============================] - 255s 284ms/step - loss: 0.3211 - acc: 0.8902 - val_loss: 1.5884 - val_acc: 0.6709\n",
      "Epoch 178/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "898/898 [==============================] - 255s 284ms/step - loss: 0.3180 - acc: 0.8922 - val_loss: 1.8755 - val_acc: 0.6668\n",
      "Epoch 179/200\n",
      "898/898 [==============================] - 255s 284ms/step - loss: 0.3209 - acc: 0.8913 - val_loss: 1.9068 - val_acc: 0.6709\n",
      "Epoch 180/200\n",
      "898/898 [==============================] - 255s 284ms/step - loss: 0.3190 - acc: 0.8894 - val_loss: 1.5858 - val_acc: 0.6668\n",
      "Epoch 181/200\n",
      "898/898 [==============================] - 255s 284ms/step - loss: 0.3201 - acc: 0.8909 - val_loss: 1.6177 - val_acc: 0.6676\n",
      "Epoch 182/200\n",
      "898/898 [==============================] - 255s 284ms/step - loss: 0.3201 - acc: 0.8917 - val_loss: 1.9108 - val_acc: 0.6668\n",
      "Epoch 183/200\n",
      "898/898 [==============================] - 255s 284ms/step - loss: 0.3209 - acc: 0.8916 - val_loss: 1.8803 - val_acc: 0.6665\n",
      "Epoch 184/200\n",
      "898/898 [==============================] - 255s 284ms/step - loss: 0.3135 - acc: 0.8942 - val_loss: 1.6217 - val_acc: 0.6695\n",
      "Epoch 185/200\n",
      "898/898 [==============================] - 255s 284ms/step - loss: 0.3174 - acc: 0.8909 - val_loss: 1.5918 - val_acc: 0.6673\n",
      "Epoch 186/200\n",
      "898/898 [==============================] - 255s 284ms/step - loss: 0.3178 - acc: 0.8911 - val_loss: 1.5858 - val_acc: 0.6679\n",
      "Epoch 187/200\n",
      "898/898 [==============================] - 255s 284ms/step - loss: 0.3176 - acc: 0.8903 - val_loss: 1.8789 - val_acc: 0.6651\n",
      "Epoch 188/200\n",
      "898/898 [==============================] - 255s 284ms/step - loss: 0.3202 - acc: 0.8899 - val_loss: 1.9108 - val_acc: 0.6698\n",
      "Epoch 189/200\n",
      "898/898 [==============================] - 255s 284ms/step - loss: 0.3249 - acc: 0.8888 - val_loss: 1.9128 - val_acc: 0.6668\n",
      "Epoch 190/200\n",
      "898/898 [==============================] - 255s 285ms/step - loss: 0.3192 - acc: 0.8920 - val_loss: 1.9128 - val_acc: 0.6656\n",
      "Epoch 191/200\n",
      "898/898 [==============================] - 255s 284ms/step - loss: 0.3192 - acc: 0.8908 - val_loss: 1.9089 - val_acc: 0.6701\n",
      "Epoch 192/200\n",
      "898/898 [==============================] - 255s 284ms/step - loss: 0.3162 - acc: 0.8931 - val_loss: 1.8755 - val_acc: 0.6656\n",
      "Epoch 193/200\n",
      "898/898 [==============================] - 255s 284ms/step - loss: 0.3203 - acc: 0.8914 - val_loss: 1.8775 - val_acc: 0.6695\n",
      "Epoch 194/200\n",
      "898/898 [==============================] - 255s 284ms/step - loss: 0.3209 - acc: 0.8903 - val_loss: 1.5912 - val_acc: 0.6673\n",
      "Epoch 195/200\n",
      "898/898 [==============================] - 255s 284ms/step - loss: 0.3185 - acc: 0.8925 - val_loss: 1.5858 - val_acc: 0.6679\n",
      "Epoch 196/200\n",
      "898/898 [==============================] - 255s 284ms/step - loss: 0.3206 - acc: 0.8894 - val_loss: 1.5892 - val_acc: 0.6687\n",
      "Epoch 197/200\n",
      "898/898 [==============================] - 255s 284ms/step - loss: 0.3180 - acc: 0.8929 - val_loss: 1.6197 - val_acc: 0.6690\n",
      "Epoch 198/200\n",
      "898/898 [==============================] - 255s 284ms/step - loss: 0.3154 - acc: 0.8943 - val_loss: 1.6203 - val_acc: 0.6698\n",
      "Epoch 199/200\n",
      "898/898 [==============================] - 255s 284ms/step - loss: 0.3207 - acc: 0.8911 - val_loss: 1.9108 - val_acc: 0.6643\n",
      "Epoch 200/200\n",
      "898/898 [==============================] - 255s 284ms/step - loss: 0.3223 - acc: 0.8890 - val_loss: 1.8769 - val_acc: 0.6648\n"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(\n",
    "    train_generator,\n",
    "    epochs=num_epochs,\n",
    "    validation_data=val_generator,\n",
    "    verbose=1,\n",
    "    callbacks=callbacks_list\n",
    ")\n",
    "\n",
    "model.save('../saves/vggFace')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fase Sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sparse(Constraint):\n",
    "    def __init__(self, mask):\n",
    "        self.mask = K.cast_to_floatx(mask)\n",
    "    \n",
    "    def __call__(self,x):\n",
    "        return self.mask * x\n",
    "    \n",
    "    def get_config(self):\n",
    "        return {'mask': self.mask}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sparsity_masks(model,sparsity):\n",
    "    weights_list = model.get_weights()\n",
    "    masks = []\n",
    "    for weights in weights_list:\n",
    "        #We can ignore biases\n",
    "        if len(weights.shape) > 1:\n",
    "            weights_abs = np.abs(weights)\n",
    "            masks.append((weights_abs>np.percentile(weights_abs,sparsity))*1.)\n",
    "    return masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masks = create_sparsity_masks(model,30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_model = Sequential([\n",
    "    Conv2D(32,3,input_shape=(28,28,1), kernel_constraint=Sparse(masks[0])),\n",
    "    Activation('relu'),\n",
    "    MaxPooling2D(2,2),\n",
    "    Dropout(0.25),\n",
    "    Conv2D(64,3, kernel_constraint=Sparse(masks[1])),\n",
    "    Activation('relu'),\n",
    "    MaxPooling2D(2,2),\n",
    "    Dropout(0.25),\n",
    "    Flatten(),\n",
    "    Dense(250, kernel_constraint=Sparse(masks[2])),\n",
    "    Activation('relu'),\n",
    "    Dropout(0.4),\n",
    "    Dense(10, kernel_constraint=Sparse(masks[3])),\n",
    "    Activation('softmax')\n",
    "])\n",
    "\n",
    "adam = Adam()\n",
    "sparse_model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "sparse_model.summary()\n",
    "#Get weights from densely trained model\n",
    "sparse_model.set_weights(model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 50\n",
    "history = sparse_model.fit_generator(\n",
    "    train_generator,\n",
    "    epochs=num_epochs,\n",
    "    validation_data=val_generator,\n",
    "    verbose=1,\n",
    "    callbacks=callbacks_list\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fase Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "redense_model = Sequential([\n",
    "    Conv2D(32,3,input_shape=(28,28,1)),\n",
    "    Activation('relu'),\n",
    "    MaxPooling2D(2,2),\n",
    "    Dropout(0.25),\n",
    "    Conv2D(64,3),\n",
    "    Activation('relu'),\n",
    "    MaxPooling2D(2,2),\n",
    "    Dropout(0.25),\n",
    "    Flatten(),\n",
    "    Dense(250),\n",
    "    Activation('relu'),\n",
    "    Dropout(0.4),\n",
    "    Dense(10),\n",
    "    Activation('softmax')\n",
    "])\n",
    "\n",
    "adam = Adam(lr=0.0001)#Default Adam lr is 0.001 so I set it to 0.0001\n",
    "redense_model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "redense_model.summary()\n",
    "#Get weights from sparsely trained model\n",
    "redense_model.set_weights(sparse_model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 50\n",
    "history = redense_model.fit_generator(\n",
    "    train_generator,\n",
    "    epochs=num_epochs,\n",
    "    validation_data=val_generator,\n",
    "    verbose=1,\n",
    "    callbacks=callbacks_list\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "FaceDetectionvggFACE",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "gputest",
   "language": "python",
   "name": "gputest"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
